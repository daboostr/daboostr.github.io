<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>WebLLM</title>
  <!-- WebLLM (MLC) CDN -->
  <script src="https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm/dist/index.min.js"></script>
  <style>
    body { font-family: system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, sans-serif; margin: 2rem; }
    textarea { width: 100%; max-width: 900px; }
    button { padding: 0.6rem 1rem; margin-top: 0.5rem; }
    .status { color: #555; margin-left: 0.5rem; min-height: 1.2em; }
    pre { background: #f6f8fa; padding: 1rem; border-radius: 8px; white-space: pre-wrap; word-wrap: break-word; max-width: 900px; }
    .row { margin-bottom: 1rem; display: flex; align-items: center; gap: 0.5rem; flex-wrap: wrap; }
    .small { font-size: 0.9rem; color: #666; }
  </style>
</head>
<body>
  <h1>Run LLM in Your Browser</h1>

  <div class="row small">
    Tip: First load downloads model weights and can take a minute (cached afterwards).
  </div>

  <div class="row">
    <label for="model">Model:</label>
    <select id="model">
      <option value="Qwen2.5-0.5B-Instruct-q4f32_1-MLC" selected>Qwen2.5-0.5B-Instruct (q4f32_1)</option>
      <option value="Phi-3-mini-4k-instruct-q4f16_1-MLC">Phi-3-mini-4k-instruct (q4f16_1)</option>
    </select>
  </div>

  <div class="row" style="flex-direction: column; align-items: stretch;">
    <textarea id="input" placeholder="Type your prompt here..." rows="6"></textarea>
  </div>

  <div class="row">
    <button id="run">Run LLM</button>
    <span class="status" id="status"></span>
  </div>

  <div class="row" style="flex-direction: column; align-items: stretch;">
    <strong>Output:</strong>
    <pre id="output"></pre>
  </div>

  <script>
    const inputEl = document.getElementById('input');
    const runBtn = document.getElementById('run');
    const outputEl = document.getElementById('output');
    const statusEl = document.getElementById('status');
    const modelSel = document.getElementById('model');

    let engine = null;
    let currentModel = null;
    let loading = false;
    let usingWorker = false;

    function setStatus(text) { statusEl.textContent = text || ''; }
    function setRunning(running) {
      runBtn.disabled = running;
      runBtn.textContent = running ? 'Running…' : 'Run LLM';
    }
    function log(...args) { try { console.log('[WebLLM]', ...args); } catch {} }

    function extractText(reply) {
      // Be robust to different library versions/fields
      if (!reply) return '';
      // Newer API shape
      let text = reply?.choices?.[0]?.message?.content;
      if (text) return text;
      // Older or alternative shapes
      text = reply?.output_text || reply?.text || reply?.choices?.[0]?.text;
      if (text) return text;
      // Best-effort stringify
      try { return JSON.stringify(reply); } catch { return String(reply); }
    }

    async function createEngineWithWorker(modelId) {
      const workerURL = '/assets/webllm/worker-proxy.js'; // same-origin path
      const worker = new Worker(workerURL, { type: 'module' });
      worker.onerror = (e) => {
        console.error('Worker error:', e);
        setStatus('Worker error. See console for details.');
      };
      const eng = await webllm.CreateWebWorkerMLCEngine(worker, modelId, {
        initProgressCallback: (progress) => {
          const pct = progress?.progress != null ? Math.round(progress.progress * 100) : 0;
          const text = progress?.text || 'Loading';
          setStatus(pct ? `${text} (${pct}%)` : text);
          log('Init progress:', progress);
        },
      });
      usingWorker = true;
      return eng;
    }

    async function createEngineMainThread(modelId) {
      const eng = await webllm.CreateMLCEngine(modelId, {
        initProgressCallback: (progress) => {
          const pct = progress?.progress != null ? Math.round(progress.progress * 100) : 0;
          const text = progress?.text || 'Loading';
          setStatus(pct ? `${text} (${pct}%)` : text);
          log('Init progress (main):', progress);
        },
      });
      usingWorker = false;
      return eng;
    }

    async function ensureEngine(modelId) {
      if (engine && currentModel === modelId) return engine;

      setStatus('Preparing engine…');

      // Try worker first; if it fails, fall back to main thread.
      try {
        engine = await createEngineWithWorker(modelId);
      } catch (e) {
        console.warn('Worker init failed, falling back to main thread.', e);
        setStatus('Worker init failed; falling back to main thread.');
        engine = await createEngineMainThread(modelId);
      }

      currentModel = modelId;
      setStatus('Model ready' + (usingWorker ? ' (worker).' : ' (main thread).'));
      return engine;
    }

    runBtn.addEventListener('click', async () => {
      const prompt = (inputEl.value || '').trim();
      if (!prompt) {
        inputEl.focus();
        return;
      }
      if (loading) return;

      loading = true;
      setRunning(true);
      outputEl.textContent = '';
      try {
        const modelId = modelSel.value;
        const eng = await ensureEngine(modelId);

        setStatus('Generating…');

        // Prefer non-stream for simplicity; handle multiple reply shapes
        const reply = await eng.chat.completions.create({
          messages: [{ role: 'user', content: prompt }],
          temperature: 0.7,
          max_tokens: 512,
        });

        const text = extractText(reply);
        outputEl.textContent = text || '(No text returned)';
        setStatus('Done.');
      } catch (err) {
        console.error(err);
        outputEl.textContent = '';
        setStatus('Error: ' + (err && err.message ? err.message : String(err)));
      } finally {
        loading = false;
        setRunning(false);
      }
    });

    if (!('gpu' in navigator)) {
      setStatus('WebGPU not detected; loading may be slower or unsupported.');
    }
  </script>
</body>
</html>
